# TOA: 


<!-- <p align="center">
  <!-- <em></em>
  <br>
   -->
  <!-- <img src="./figures/mas.png" alt="" width="400">
</p> -->

<!-- [![arXiv](https://img.shields.io/badge/arXiv-paper-b31b1b.svg)](https://arxiv.org/pdf/2412.17061)
   -->

<div style="text-align: center;">
  <img src="./figures/mas.png" alt="" width="400">
  <br>
  <a href="https://arxiv.org/pdf/2412.17061" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/arXiv-paper-b31b1b.svg" alt="arXiv Paper">
  </a>
</div>

<br>

This is officical repository for the work [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/pdf/2412.17061). We study how to synthesize data for alignment from multiple distinct language models such as Llama3, Qwen2, Mistral, etc, which is so called problem of multi-agent sampling. We propose [TOA]() (Tree Search-based Orchestrated Agents) to achieve this goal. Our method is driven by Monte Carlo Tree Search with a Reward Model integrated. 


**TOA** is designed to **synthesize alignment data** (specifically the output responses) from a diverse range of language models. 


## ðŸŒŸ Key Features

- ðŸ”“ **Open-source models**: [Llama Series](https://huggingface.co/meta-llama), [Qwen Series](https://huggingface.co/Qwen), [Mistral Series](https://huggingface.co/mistralai), and more.
- ðŸ”’ **Closed-source models**: OpenAI, Claude, etc.
- ðŸ˜Š **OpenAI Compatible Server**: We support OpenAI compatible API to use the models. 
- ðŸŽ¯ **Reward Model Integration**: TOA utilizes a reward model to guide and optimize the generation process. You can easily specifiy a your own reward model.
- ðŸ’° **Compute Efficient**: For each input question, TOA optimizes the generation structure dynamically with MCTS-based search, making our method more compute-efficient than other baselines for data synthesis. 
- ðŸ“£ **Support Various Methods**: Our repository implements various methods for single- and multi-agent sampling.


## Supported Methods
| Method              | Paper        | Example Code |
|---------------------|--------------|--------------|
| Random Sampling     | [Link]()     | -            |
| PRS                 | [Link]()     | -            |
| Parallel Ensemble   | [Link]()     | -            |
| Sequential Refine   | [Link]()     | -            |
| MoA                 | [Link]()     | -            |
| TOA (ours)          | [Link]()     | -            |



## Project Directory Structure

- **README.md**
  - Provides an introduction to the project, usage instructions, and documentation.

- **LICENSE**
  - Contains the project license, outlining the rules and permissions for using the project.

- **.DS_Store**
  - A temporary file automatically generated by macOS. It's not part of the core project and should be ignored.

- **bash/**
  - Contains scripts for automation, environment setup, or other command-line utilities.

- **chat_templates/**
  - Includes predefined templates for dialogues or interactions, likely used in Natural Language Processing (NLP) tasks.

- **code/**
  - The core code directory, containing scripts for training, testing, and evaluating models.

- **data/**
  - Stores project datasets, including training, testing, or intermediate processed data.

- **figures/**
  - A directory for storing visualizations, such as charts, graphs, or results.

- **model_configs/**
  - Contains model configuration files, such as parameter settings and architecture definitions.

- **useful_code/**
  - Holds utility scripts or helpful tools to support the project, such as debugging or preprocessing tools.




## News
- [2024/12/22] [TOA paper](https://arxiv.org/pdf/2412.17061) is out at arXiv. 



## Quick Start

### 1. Start Local Servers
If you want to host the language models locally, you can use the provide the code to start the local servers. 

```bash
cd bash/launch_large_models

python start_server.vllm.py path_to_config root_to_save GPU port gpu_utilize
```


- path_to_config: path to the configuration file of the model, which is in JSON format and looks like
```bash
{
    "policy_model": {
            "llama-3.1-8b-instruct": {
                "path_to_model": "",
                "path_to_chat_template": "../chat_templates/llama-3.1-instruct.jinja",
                "stop_tokens": "['<|eot_id|>']"
        }
    }
}
```
- root_to_save: path to save the server configuration, which is in JSON format and looks like:
```bash

    "model_name": "llama-3.1-8b-instruct",
    "config": {
        "path_to_model": "",
        "path_to_chat_template": "../chat_templates/llama-3.1-instruct.jinja",
        "stop_tokens": "['<|eot_id|>']",
        "api_key": "abc123",
        "port": e.g., 8000,
        "host": the local machine address,
        "GPU": e.g., "0",
        "gpu_utilize": e.g., 0.9
    }
}
```
- GPU: gpu ids, such as "0", "0,1", "0,1,2,3"
- port: 8000, 8001, etc
- gpu_utilize: how much gpu memory to use, such as 0.9, 0.8

You can start the server for different models, just make sure to save all the server configuration into one same folder, that is [root_to_save](). 


### 2. Reward Model
A reward model is needed to generate rewards for generated responses in the real time. 

We need to specify the configurations for the reward model in the JSON format, 
```bash
cd model_configs
```
which is like:
```bash
{
    "reward_model": {"name": "ArmoRM", "path": "", "GPU": "0" }
}
```

For the personalized model, you may need to revise the code [code/reward.py](./code/reward.py). You need to specify how the reward model can be used for reward calculation. 



### 3. Start Data Synthesis
Now, we can start data generation!
```bash
cd bash
cd exp_alpaca_eval
bash run_generate.api.mcts.pre_load.sh
```








![](./figures/method.png)



