# TOA: 


<!-- <p align="center">
  <!-- <em></em>
  <br>
   -->
  <!-- <img src="./figures/mas.png" alt="" width="400">
</p> -->

<!-- [![arXiv](https://img.shields.io/badge/arXiv-paper-b31b1b.svg)](https://arxiv.org/pdf/2412.17061)
   -->

<div style="text-align: center;">
  <img src="./figures/mas.png" alt="" width="400">
  <br>
  <a href="https://arxiv.org/pdf/2412.17061" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/arXiv-paper-b31b1b.svg" alt="arXiv Paper">
  </a>
</div>

<br>

This is officical repository for the work [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/pdf/2412.17061). We study how to synthesize data for alignment from multiple distinct language models such as Llama3, Qwen2, Mistral, etc, which is so called problem of multi-agent sampling. We propose [TOA]() (Tree Search-based Orchestrated Agents) to achieve this goal. Our method is driven by Monte Carlo Tree Search with a Reward Model integrated. 


ðŸŒŸ Key Features

	â€¢	Broad Compatibility: TOA supports:
	â€¢	Open-source models: Llama Series, Qwen Series, Mistral Series, and more.
	â€¢	Closed-source models: OpenAI (ChatGPT), Claude, etc.
	â€¢	Reward Model Integration: TOA utilizes a reward model to guide and optimize the generation process, ensuring quality and relevance.








## News
- [2024/12/22] [TOA paper](https://arxiv.org/pdf/2412.17061) is out at arXiv. 



## Quick Start














![](./figures/method.png)



